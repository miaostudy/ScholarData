import os
import json
import re
import requests
import hashlib
import threading
from datetime import datetime
from collections import defaultdict, Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

plt.rcParams["font.family"] = ["SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


class PaperKnowledgeGraphBuilder:
    def __init__(self, papers_dir, glm_api_key, cache_dir="paper_knowledge_graph_cache"):
        self.papers_dir = os.path.abspath(papers_dir)
        self.glm_api_key = glm_api_key
        self.cache_dir = cache_dir

        # ç®€åŒ–ç¼“å­˜æ–‡ä»¶ï¼šåªä¿ç•™å¿…è¦çš„ç¼“å­˜ï¼Œå»æ‰å…¨å±€ç¼“å­˜keyç›¸å…³
        self.paper_cache = os.path.join(cache_dir, "paper_cache.json")  # è®ºæ–‡æ•°æ®ç¼“å­˜ï¼ˆkey: paper_idï¼‰
        self.keywords_cache = os.path.join(cache_dir, "keywords_cache.json")  # å…³é”®è¯ç¼“å­˜
        self.relations_cache = os.path.join(cache_dir, "relations_cache.json")  # å…³ç³»ç¼“å­˜

        # åˆå§‹åŒ–ç¼“å­˜ç›®å½•å’Œæ–‡ä»¶
        self._init_cache()

        # åŠ è½½ç¼“å­˜ï¼ˆç›´æ¥ç”¨è®ºæ–‡IDä½œä¸ºkeyï¼Œé¿å…é‡å¤ï¼‰
        self.paper_data = self._load_cache(self.paper_cache)  # æ ¼å¼ï¼š{paper_id: paper_info}
        self.keywords_data = self._load_cache(self.keywords_cache)
        self.relations_data = self._load_cache(self.relations_cache)

        # ç®€åŒ–çº¿ç¨‹é”
        self.cache_locks = {
            "paper": threading.Lock(),
            "keywords": threading.Lock(),
            "relations": threading.Lock()
        }

    def _init_cache(self):
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•å’Œç©ºç¼“å­˜æ–‡ä»¶"""
        os.makedirs(self.cache_dir, exist_ok=True)
        for cache_file in [self.paper_cache, self.keywords_cache, self.relations_cache]:
            if not os.path.exists(cache_file):
                with open(cache_file, "w", encoding="utf-8") as f:
                    json.dump({}, f, ensure_ascii=False, indent=2)

    def _load_cache(self, file_path):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶ï¼Œå¼‚å¸¸æ—¶è¿”å›ç©ºå­—å…¸"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½ç¼“å­˜ {os.path.basename(file_path)} å¤±è´¥ï¼š{str(e)}ï¼Œä½¿ç”¨ç©ºå­—å…¸")
            return {}

    def _save_cache(self, data, file_path, lock_name):
        """å®æ—¶ä¿å­˜ç¼“å­˜ï¼ˆå•ç¯‡è®ºæ–‡å¤„ç†å®Œç«‹å³ä¿å­˜ï¼‰"""
        lock = self.cache_locks.get(lock_name)
        if not lock:
            print(f"æ— æ•ˆçš„é”åç§°ï¼š{lock_name}")
            return False
        with lock:
            try:
                temp_path = f"{file_path}.tmp"
                with open(temp_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                os.replace(temp_path, file_path)  # åŸå­æ“ä½œ
                return True
            except Exception as e:
                print(f"ä¿å­˜ç¼“å­˜ {os.path.basename(file_path)} å¤±è´¥ï¼š{str(e)}")
                return False

    def load_paper_data(self, force_refresh=False):
        """åŠ è½½è®ºæ–‡æ•°æ®ï¼šéå†JSONæ–‡ä»¶ï¼Œå¤„ç†ä¸€ç¯‡ç¼“å­˜ä¸€ç¯‡ï¼Œé¿å…é‡å¤"""
        print(f"æ­£åœ¨è¯»å–è®ºæ–‡æ–‡ä»¶å¤¹ï¼š{self.papers_dir}")

        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        all_json_files = [f for f in os.listdir(self.papers_dir) if f.endswith(".json")]
        total_files = len(all_json_files)
        print(f"å‘ç° {total_files} ç¯‡è®ºæ–‡æ–‡ä»¶")

        processed_count = 0
        valid_count = 0
        invalid_count = 0

        for idx, filename in enumerate(all_json_files, 1):
            file_path = os.path.join(self.papers_dir, filename)
            paper_id = filename.replace(".json", "")

            # è·³è¿‡å·²å¤„ç†çš„è®ºæ–‡ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
            if not force_refresh and paper_id in self.paper_data:
                print(f"[{idx}/{total_files}] è®ºæ–‡ {paper_id} å·²ç¼“å­˜ï¼Œè·³è¿‡")
                processed_count += 1
                valid_count += 1
                continue

            try:
                # è¯»å–å•ç¯‡è®ºæ–‡
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # æå–æ ¸å¿ƒå­—æ®µ
                paper_info = {
                    "paper_id": paper_id,
                    "title": data.get("title", "Unknown Title"),
                    "abstract": data.get("abstract", "").strip(),
                    "authors": data.get("authors", []),
                    "publication_date": data.get("publication_date", ""),
                    "year": self._parse_year(data.get("publication_date", "")),
                    "ieee_keywords": self._standardize_keywords(data.get("ieee_keywords", [])),
                    "index_terms": self._standardize_keywords(data.get("index_terms", [])),
                    "author_keywords": self._standardize_keywords(data.get("author_keywords", [])),
                }
                # åˆå¹¶æ‰€æœ‰å…³é”®è¯
                paper_info["all_keywords"] = list(set(
                    paper_info["ieee_keywords"] + paper_info["index_terms"] + paper_info["author_keywords"]
                ))

                # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå¤„ç†ä¸€ç¯‡ä¿å­˜ä¸€ç¯‡ï¼‰
                self.paper_data[paper_id] = paper_info
                self._save_cache(self.paper_data, self.paper_cache, "paper")

                print(f"[{idx}/{total_files}] æˆåŠŸè¯»å–è®ºæ–‡ {paper_id}ï¼ˆæ ‡é¢˜ï¼š{paper_info['title'][:50]}...ï¼‰")
                valid_count += 1
                processed_count += 1

            except Exception as e:
                print(f"[{idx}/{total_files}] è¯»å–è®ºæ–‡ {paper_id} å¤±è´¥ï¼š{str(e)}")
                invalid_count += 1

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼è¿”å›ï¼ˆä¾¿äºåç»­å¤„ç†ï¼‰
        paper_list = list(self.paper_data.values())
        print(f"\nè®ºæ–‡æ•°æ®è¯»å–å®Œæˆï¼šæœ‰æ•ˆè®ºæ–‡ {valid_count} ç¯‡ï¼Œæ— æ•ˆè®ºæ–‡ {invalid_count} ç¯‡ï¼Œå·²ç¼“å­˜ {len(paper_list)} ç¯‡")
        return paper_list

    def _standardize_keywords(self, keywords):
        """å…³é”®è¯æ ‡å‡†åŒ–"""
        if not isinstance(keywords, list):
            return []
        standardized = []
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                standardized_kw = kw.strip().lower()
                if standardized_kw not in standardized:
                    standardized.append(standardized_kw)
        return standardized

    def _parse_year(self, date_str):
        """è§£æå¹´ä»½"""
        if not date_str or not isinstance(date_str, str):
            return None
        match = re.search(r"\b(19|20)\d{2}\b", date_str)
        return int(match.group()) if match else None

    def _extract_keywords_from_paper(self, paper):
        """å•ç¯‡è®ºæ–‡å…³é”®è¯æå–ï¼ˆä¿®å¤ç©ºå“åº”è§£æé”™è¯¯ï¼‰"""
        paper_id = paper["paper_id"]
        title = paper["title"]
        abstract = paper["abstract"]
        existing_kw = paper["all_keywords"]  # å·²æœ‰çš„å…³é”®è¯ï¼ˆä»…APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰

        # 1. å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œç›´æ¥è¿”å›å·²æœ‰å…³é”®è¯
        if not abstract:
            print(f"è®ºæ–‡ {paper_id} æ— æ‘˜è¦ï¼Œè¿”å›å·²æœ‰å…³é”®è¯")
            return existing_kw

        # 2. æç¤ºè¯ï¼šæ˜ç¡®ä¼˜å…ˆè¾“å‡ºJSONï¼Œé¿å…æ€è€ƒè¿‡ç¨‹è¿‡é•¿
        prompt = f"""
        Task: Extract core academic keywords from the title and abstract (if any).
        Research Field: All academic fields (adapt to the paper's actual field automatically)

        Keyword Requirements:
        1. Accuracy: Strictly based on the content, no irrelevant keywords
        2. Conciseness: Use single terms or short phrases (2-4 words), avoid long sentences
        3. Consistency: Use standard academic terminology, prefer well-known abbreviations (e.g., "nlp" instead of "natural language processing")
        4. Quantity: No limit (can be 0 if no valid keywords can be extracted)

        Output Format:
        ONLY return JSON, no extra text, no comments, no reasoning.
        Example (with keywords): {{"keywords": ["transformer", "natural language processing"]}}
        Example (no keywords): {{"keywords": []}}

        Paper Content:
        Title: {title}
        Abstract: {abstract[:1000]}  # æˆªæ–­è¿‡é•¿æ‘˜è¦ï¼Œé¿å…æç¤ºè¯å ç”¨è¿‡å¤šToken
        """

        # 3. ç³»ç»Ÿæç¤ºè¯ï¼šå¼ºè°ƒåªè¾“å‡ºJSONï¼Œä¸é¢å¤–å†…å®¹
        system_msg = """
        You are a professional academic keyword extraction expert.
        Extract keywords strictly based on the paper content.
        ONLY output JSON format as required, no extra reasoning or explanation.
        If no valid keywords can be found, return {{"keywords": []}}.
        """

        # 4. APIè°ƒç”¨ï¼šä»…å¼‚å¸¸æ—¶é‡è¯•
        response = None
        retry_count = 0
        max_retries = 3
        while retry_count < max_retries:
            try:
                response = self._call_glm_api(prompt, system_message=system_msg.strip())
                break
            except Exception as e:
                retry_count += 1
                print(f"è®ºæ–‡ {paper_id} å…³é”®è¯æå–é‡è¯• {retry_count}/{max_retries}ï¼š{str(e)}")

        # 5. å¤„ç†APIè°ƒç”¨å¤±è´¥
        if response is None:
            print(f"è®ºæ–‡ {paper_id} å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¿”å›å·²æœ‰å…³é”®è¯")
            return existing_kw

        # 6. ä¿®å¤ç©ºå“åº”è§£æé”™è¯¯ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
        try:
            # å…ˆåˆ¤æ–­å“åº”æ˜¯å¦ä¸ºç©º
            response = response.strip().strip("`").strip()
            if not response:  # å“åº”ä¸ºç©ºå­—ç¬¦ä¸²
                print(f"è®ºæ–‡ {paper_id} å¤§æ¨¡å‹è¿”å›ç©ºå“åº”ï¼Œè¿”å›ç©ºå…³é”®è¯åˆ—è¡¨")
                return []  # æŒ‰éœ€æ±‚è¿”å›ç©ºåˆ—è¡¨ï¼Œä¹Ÿå¯æ”¹ä¸º return existing_kw
            if response.startswith("json"):
                response = response[4:].strip()
            result = json.loads(response)
            extracted_kw = self._standardize_keywords(result.get("keywords", []))
            print(f"è®ºæ–‡ {paper_id} æå–å…³é”®è¯ {len(extracted_kw)} ä¸ªï¼š{extracted_kw[:5]}...")
            return extracted_kw
        except Exception as e:
            print(f"è®ºæ–‡ {paper_id} è§£æå…³é”®è¯å¤±è´¥ï¼š{str(e)}ï¼Œè¿”å›å·²æœ‰å…³é”®è¯")
            return existing_kw

    def extract_and_merge_keywords(self, paper_list, force_refresh=False):
        """æå–+åˆå¹¶å…³é”®è¯"""
        cache_key = "all_keywords"
        if not force_refresh and cache_key in self.keywords_data:
            print("ä»ç¼“å­˜åŠ è½½å…³é”®è¯æ•°æ®")
            return self.keywords_data[cache_key]

        print(f"å¼€å§‹å¤„ç†å…³é”®è¯ï¼ˆå…± {len(paper_list)} ç¯‡è®ºæ–‡ï¼‰")
        keywords_counter = Counter()

        # åˆå§‹åŒ–å•ç¯‡è®ºæ–‡å…³é”®è¯ç¼“å­˜
        if "paper_keywords" not in self.keywords_data:
            self.keywords_data["paper_keywords"] = {}

        for idx, paper in enumerate(paper_list, 1):
            paper_id = paper["paper_id"]
            title = paper["title"]

            # ä¼˜å…ˆä½¿ç”¨å·²ç¼“å­˜çš„æå–å…³é”®è¯
            if not force_refresh and paper_id in self.keywords_data["paper_keywords"]:
                extracted_kw = self.keywords_data["paper_keywords"][paper_id]
                print(f"[{idx}/{len(paper_list)}] è®ºæ–‡ {paper_id} å·²ç¼“å­˜å…³é”®è¯ï¼ˆ{len(extracted_kw)}ä¸ªï¼‰ï¼Œè·³è¿‡")
            else:
                print(f"[{idx}/{len(paper_list)}] æ­£åœ¨æå–è®ºæ–‡ {paper_id} å…³é”®è¯ï¼ˆæ ‡é¢˜ï¼š{title[:50]}...ï¼‰")
                extracted_kw = self._extract_keywords_from_paper(paper)

                # ç¼“å­˜å•ç¯‡è®ºæ–‡çš„æå–å…³é”®è¯
                self.keywords_data["paper_keywords"][paper_id] = extracted_kw
                self._save_cache(self.keywords_data, self.keywords_cache, "keywords")

            # ç»Ÿè®¡å…³é”®è¯ï¼ˆå»é‡ï¼‰
            for kw in set(extracted_kw):
                keywords_counter[kw] += 1

            # è¿›åº¦æç¤º
            if idx % 10 == 0 or idx == len(paper_list):
                print(f"[{idx}/{len(paper_list)}] å·²å¤„ç† {idx} ç¯‡è®ºæ–‡ï¼Œç´¯è®¡æå–å…³é”®è¯ç±»å‹ {len(keywords_counter)} ä¸ª")

        # åˆå¹¶ç›¸ä¼¼å…³é”®è¯
        merged_keywords = self._merge_similar_keywords(dict(keywords_counter))

        # ç¼“å­˜æœ€ç»ˆå…³é”®è¯
        self.keywords_data[cache_key] = merged_keywords
        self._save_cache(self.keywords_data, self.keywords_cache, "keywords")

        print(f"\nå…³é”®è¯å¤„ç†å®Œæˆï¼šå…± {len(merged_keywords)} ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼ˆTop10ï¼š{list(merged_keywords.keys())[:10]}ï¼‰")
        return merged_keywords

    def _merge_similar_keywords(self, keywords):
        """åˆå¹¶ç›¸ä¼¼å…³é”®è¯"""
        if len(keywords) < 2:
            return keywords

        prompt = f"""
        Task: Merge semantically similar academic keywords, retain the most representative term, sum their weights.
        Research Field: All academic fields (adapt to the keywords' actual field)

        Requirements:
        1. Merge only highly similar keywords (e.g., "nlp" â†” "natural language processing", "cnn" â†” "convolutional neural network")
        2. Do NOT merge unrelated keywords (e.g., "image processing" â†” "text classification" are not merged)
        3. Representative term: Prefer concise, well-known terms or abbreviations (e.g., use "nlp" instead of "natural language processing")
        4. Weight calculation: Sum the weights of all merged keywords (do not change the total weight)
        5. Completeness: Do not delete any keywords (all input keywords must be merged into some representative term)
        6. Output format: Strict JSON, no extra text. Example:
           {{"merged_keywords": [{{"word": "nlp", "weight": 15}}, {{"word": "cnn", "weight": 12}}]}}

        Input Keywords (word: weight):
        """
        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        for word, weight in sorted_keywords[:50]:
            prompt += f"- {word}: {weight}\n"

        if len(sorted_keywords) > 50:
            extra_kw = [word for word, _ in sorted_keywords[50:]]
            prompt += f"- Additional keywords (low weight): {', '.join(extra_kw[:20])}...\n"

        system_msg = """
        You are a professional academic keyword merging expert.
        Your task is to merge semantically similar keywords while maintaining the core meaning and weight.
        Ensure that the merged keywords are representative and widely used in academic circles.
        If you are unsure whether two keywords are similar, do NOT merge them.
        """

        response = self._call_glm_api(prompt, system_message=system_msg.strip())
        if not response:
            print("å…³é”®è¯åˆå¹¶å¤±è´¥ï¼Œè¿”å›åŸå§‹å…³é”®è¯")
            return keywords

        try:
            response = response.strip().strip("`").strip()
            if response.startswith("json"):
                response = response[4:].strip()
            result = json.loads(response)
            merged = {}
            for item in result.get("merged_keywords", []):
                word = item.get("word", "").strip().lower()
                weight = item.get("weight", 0)
                if word and weight > 0:
                    merged[word] = weight

            if len(merged) < max(5, len(keywords) // 3):
                print("å…³é”®è¯åˆå¹¶ç»“æœå¼‚å¸¸ï¼Œè¿”å›åŸå§‹å…³é”®è¯")
                return keywords

            return dict(sorted(merged.items(), key=lambda x: x[1], reverse=True))
        except Exception as e:
            print(f"è§£æåˆå¹¶å…³é”®è¯ç»“æœå¤±è´¥ï¼š{str(e)}ï¼Œè¿”å›åŸå§‹å…³é”®è¯")
            return keywords

    def _call_glm_api(self, prompt, system_message=None):
        """è°ƒç”¨GLM APIï¼ˆè°ƒæ•´max_tokensé¿å…æˆªæ–­ï¼‰"""
        api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.glm_api_key}",
            "Content-Type": "application/json"
        }

        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt.strip()})

        payload = {
            "model": "glm-4.5v",
            "messages": messages,
            "temperature": 0.3,
            "response_format": {"type": "json_object"},
            "timeout": 300,
            "max_tokens": 768
        }

        try:
            response = requests.post(
                api_url,
                headers=headers,
                data=json.dumps(payload, ensure_ascii=False),
                timeout=300
            )
            response.raise_for_status()
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"]
            print("APIè¿”å›ç»“æœä¸ºç©º")
            return ""
        except Exception as e:
            print(f"APIè°ƒç”¨å¼‚å¸¸ï¼š{str(e)}")
            raise

    def build_entities_and_relations(self, paper_list, merged_keywords, force_refresh=False):
        """æ„å»ºå®ä½“å’Œå…³ç³»"""
        if not force_refresh and "all_entities" in self.relations_data and "all_relations" in self.relations_data:
            print("ä»ç¼“å­˜åŠ è½½å®ä½“å’Œå…³ç³»æ•°æ®")
            return self.relations_data["all_entities"], self.relations_data["all_relations"]

        print("å¼€å§‹æ„å»ºå®ä½“å’Œå…³ç³»...")

        entities = {
            "Keyword": [],
            "Paper": [],
            "Author": []
        }

        for word, weight in merged_keywords.items():
            entities["Keyword"].append({
                "id": f"kw_{hashlib.md5(word.encode()).hexdigest()[:8]}",
                "name": word,
                "weight": weight,
                "description": f"Core research concept (frequency: {weight})"
            })

        for paper in paper_list:
            entities["Paper"].append({
                "id": f"paper_{hashlib.md5(paper['paper_id'].encode()).hexdigest()[:8]}",
                "title": paper["title"],
                "abstract": paper["abstract"][:200] + "..." if len(paper["abstract"]) > 200 else paper["abstract"],
                "year": paper["year"],
                "authors": [a.strip() for a in paper["authors"] if a.strip()]
            })

        # ä½œè€…å®ä½“ï¼ˆå»é‡ï¼‰
        author_papers = defaultdict(list)
        paper_id_map = {p["title"]: p["id"] for p in entities["Paper"]}
        for paper in paper_list:
            paper_id = paper_id_map.get(paper["title"])
            if not paper_id:
                continue
            for author in paper["authors"]:
                author = author.strip()
                if author:
                    author_papers[author].append(paper_id)

        for author, papers in author_papers.items():
            entities["Author"].append({
                "id": f"author_{hashlib.md5(author.encode()).hexdigest()[:8]}",
                "name": author,
                "paper_count": len(papers),
                "affiliated_papers": papers
            })

        # æ„å»ºå…³ç³»
        relations = []
        kw_name_to_id = {kw["name"]: kw["id"] for kw in entities["Keyword"]}
        paper_title_to_id = {p["title"]: p["id"] for p in entities["Paper"]}
        author_name_to_id = {a["name"]: a["id"] for a in entities["Author"]}

        # å…³é”®è¯-è®ºæ–‡å…³è”
        for paper in paper_list:
            paper_id = paper_title_to_id.get(paper["title"])
            if not paper_id:
                continue
            extracted_kw = self.keywords_data["paper_keywords"].get(paper["paper_id"], [])
            for kw in extracted_kw:
                kw_id = kw_name_to_id.get(kw)
                if kw_id:
                    relations.append({
                        "source_id": kw_id,
                        "target_id": paper_id,
                        "relation_type": "related_to_paper",
                        "attributes": {"description": f"Keyword '{kw}' related to paper"}
                    })

        # ä½œè€…-å…³é”®è¯å…³è”
        for paper in paper_list:
            paper_id = paper_title_to_id.get(paper["title"])
            if not paper_id:
                continue
            extracted_kw = self.keywords_data["paper_keywords"].get(paper["paper_id"], [])
            paper_kw = [kw for kw in extracted_kw if kw in kw_name_to_id]
            for author in paper["authors"]:
                author = author.strip()
                author_id = author_name_to_id.get(author)
                if author_id:
                    for kw_id in [kw_name_to_id[kw] for kw in paper_kw]:
                        relations.append({
                            "source_id": author_id,
                            "target_id": kw_id,
                            "relation_type": "researches_on",
                            "attributes": {"description": f"Author researches on keyword '{kw}'"}
                        })

        # å…³é”®è¯å…±ç°å…³ç³»
        co_occur_counter = defaultdict(int)
        for paper in paper_list:
            extracted_kw = self.keywords_data["paper_keywords"].get(paper["paper_id"], [])
            paper_kw = [kw for kw in extracted_kw if kw in kw_name_to_id]
            for i in range(len(paper_kw)):
                for j in range(i + 1, len(paper_kw)):
                    kw1, kw2 = paper_kw[i], paper_kw[j]
                    pair_key = tuple(sorted([kw1, kw2]))
                    co_occur_counter[pair_key] += 1

        for (kw1, kw2), count in co_occur_counter.items():
            if count >= 2:
                relations.append({
                    "source_id": kw_name_to_id[kw1],
                    "target_id": kw_name_to_id[kw2],
                    "relation_type": "co_occurrence",
                    "attributes": {"count": count, "description": f"Co-occur {count} times in papers"}
                })

        # ç¼“å­˜å®ä½“å’Œå…³ç³»
        self.relations_data["all_entities"] = entities
        self.relations_data["all_relations"] = relations
        self._save_cache(self.relations_data, self.relations_cache, "relations")

        print(f"å®ä½“å’Œå…³ç³»æ„å»ºå®Œæˆï¼š")
        print(
            f" - å®ä½“ï¼šå…³é”®è¯ {len(entities['Keyword'])} ä¸ª | è®ºæ–‡ {len(entities['Paper'])} ç¯‡ | ä½œè€… {len(entities['Author'])} ä½")
        print(f" - å…³ç³»ï¼šå…± {len(relations)} æ¡")
        return entities, relations

    def generate_wordcloud(self, keywords, title, output_filename):
        """ç”Ÿæˆè¯äº‘å›¾"""
        if not keywords:
            print("æ²¡æœ‰å…³é”®è¯å¯ç”Ÿæˆè¯äº‘")
            return

        filtered_kw = {kw: weight for kw, weight in keywords.items() if weight >= 2}
        if not filtered_kw:
            print("æ²¡æœ‰è¶³å¤Ÿæƒé‡çš„å…³é”®è¯ï¼ˆéœ€â‰¥2ï¼‰")
            return

        wordcloud = WordCloud(
            width=1200, height=800,
            background_color="white",
            max_words=50,
            contour_width=2,
            contour_color="steelblue",
            random_state=42
        ).generate_from_frequencies(filtered_kw)

        plt.figure(figsize=(15, 10))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.title(title, fontsize=20, pad=20, fontweight="bold")

        output_path = os.path.join(self.cache_dir, output_filename)
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"è¯äº‘å›¾å·²ä¿å­˜è‡³ï¼š{output_path}")

    def build_knowledge_graph(self, force_refresh=False):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("=" * 80)
        print("å¼€å§‹æ„å»ºè®ºæ–‡çŸ¥è¯†å›¾è°±")
        print("=" * 80)

        # æ­¥éª¤1ï¼šåŠ è½½è®ºæ–‡æ•°æ®
        paper_list = self.load_paper_data(force_refresh=force_refresh)
        if not paper_list:
            print("æ²¡æœ‰æœ‰æ•ˆè®ºæ–‡æ•°æ®ï¼Œç»ˆæ­¢æ„å»º")
            return None

        # æ­¥éª¤2ï¼šæå–å¹¶åˆå¹¶å…³é”®è¯
        merged_keywords = self.extract_and_merge_keywords(paper_list, force_refresh=force_refresh)
        self.generate_wordcloud(merged_keywords, "æ ¸å¿ƒå…³é”®è¯è¯äº‘", "keywords_wordcloud.png")

        # æ­¥éª¤3ï¼šæ„å»ºå®ä½“å’Œå…³ç³»
        entities, relations = self.build_entities_and_relations(paper_list, merged_keywords,
                                                                force_refresh=force_refresh)

        # æ„å»ºæœ€ç»ˆçŸ¥è¯†å›¾è°±
        knowledge_graph = {
            "metadata": {
                "build_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "paper_count": len(paper_list),
                "entity_count": {
                    "Keyword": len(entities["Keyword"]),
                    "Paper": len(entities["Paper"]),
                    "Author": len(entities["Author"])
                },
                "relation_count": len(relations),
                "source_folder": self.papers_dir
            },
            "entities": entities,
            "relations": relations
        }

        # ä¿å­˜æœ€ç»ˆç»“æœ
        kg_path = os.path.join(self.cache_dir, "paper_knowledge_graph.json")
        with open(kg_path, "w", encoding="utf-8") as f:
            json.dump(knowledge_graph, f, ensure_ascii=False, indent=2)
        print(f"\nçŸ¥è¯†å›¾è°±å·²ä¿å­˜è‡³ï¼š{kg_path}")

        # è¾“å‡ºæ‘˜è¦
        print("\n" + "=" * 80)
        print("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š æ¦‚è§ˆï¼š{len(paper_list)} ç¯‡è®ºæ–‡ | {len(entities['Keyword'])} ä¸ªå…³é”®è¯ | {len(relations)} æ¡å…³ç³»")
        print("=" * 80)

        return knowledge_graph


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    PAPERS_DIR = "../ieee/json_cache/papers"
    GLM_API_KEY = "38ef8158834549efa2404f4cb748cf73.fO94Wjp0BxJ80a1T"
    CACHE_DIR = "paper_knowledge_graph_cache"
    FORCE_REFRESH = False  # å¿…é¡»è®¾ä¸ºTrueï¼Œé‡æ–°å¤„ç†ä¹‹å‰å¤±è´¥çš„è®ºæ–‡

    # éªŒè¯å‚æ•°
    if not os.path.exists(PAPERS_DIR):
        print(f"é”™è¯¯ï¼šè®ºæ–‡æ–‡ä»¶å¤¹ {PAPERS_DIR} ä¸å­˜åœ¨")
        exit(1)
    if not GLM_API_KEY or GLM_API_KEY.startswith("your_"):
        print("é”™è¯¯ï¼šè¯·å¡«å†™æœ‰æ•ˆçš„GLM APIå¯†é’¥")
        exit(1)

    # æ‰§è¡Œæ„å»º
    kg_builder = PaperKnowledgeGraphBuilder(PAPERS_DIR, GLM_API_KEY, CACHE_DIR)
    kg_builder.build_knowledge_graph(FORCE_REFRESH)